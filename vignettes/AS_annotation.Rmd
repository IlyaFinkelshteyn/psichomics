---
title: "Splicing event annotation"
author: "Nuno Agostinho"
date: "`r Sys.Date()`"
output: 
  html_document: 
    toc: yes
vignette: >
  %\VignetteIndexEntry{Splicing event annotation}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

The splicing events annotation is different according to the program that
creates it. The programs which allow to quantify expression levels of
alternative splicing events studied in this report are:

* [MISO](https://miso.readthedocs.org/en/fastmiso/)
* [SUPPA](https://bitbucket.org/regulatorygenomicsupf/suppa)
* [VAST-TOOLS](https://github.com/vastgroup/vast-tools)
* [MATS](http://rnaseq-mats.sourceforge.net)

It's also important to understand that the splicing events annotation may vary
according to the splicing event type. The most common splicing event types
annotated across the aforementioned programs are described in the following
image (retrieved from the
[SUPPA website](https://bitbucket.org/regulatorygenomicsupf/suppa)).

![Splicing event types](https://bitbucket-assetroot.s3.amazonaws.com/repository/4gEBMd/1362804745-Slide2.jpg?Signature=OjMcjIPORDWJxu0EAeO9SMl%2BDnA%3D&Expires=1451777771&AWSAccessKeyId=0EMWEFSGA12Z1HF1TZ82)

To conciliate the output from the different programs, it was decided to parse
each splicing event ID to a R data type known as list with all the important
attributes of an event, including:

* Species
* Chromosome
* Strand
* Gene
* mRNA
* Splicing event type
* Inclusion level (PSI)

In order to store the events themselves, it's possible to characterise all the
represented splicing event types by using the boundaries of the following 4
exons:

* Upstream constitutive exon (C1)
* Upstram alternative exon (A1)
* Downstream alternative exon (A2)
* Downstream constitutive exon (C2)

The following table describes which positions should be stored to identify each
splicing event type:

Splicing event type        | C1 start | C1 end | A1 start | A1 end | A2 start | A2 end | C2 start | C2 end |
-------------------------- | :------: | :----: | :------: | :----: | :------: | :----: | :------: | :----: |
Exon skipping              |          | •      | •        | •      |          |        | •        |        |
Mutually exclusive exons   |          | •      | •        | •      | •        | •      | •        |        |
Alternative 5' splice site |          | • •    |          |        |          |        | •        |        |
Alternative 3' splice site |          | •      |          |        |          |        | • •      |        |
Retained intron            | •        | •      |          |        |          |        | •        | •      |
Alternative first exon     | •        | •      | •        | •      |          |        | •        |        |
Alternative last exon      |          | •      | •        | •      |          |        | •        | •      | 
Tandem UTR                 |          |        |          |        |          |        | •        | • •    | 

Note there are two glyphs in some cases. That means two alternative positions
are stored (for example, in the case of the alternative 5' splice site, the C1
end can alternatively be one of the two positions).

Before discussing how to parse thousands and thousands of events, we should
reflect a bit on the tools available to compare different solutions that solve
the same problem and how to measure performance on R. You can read about it
in the article [Measuring performance](Measuring performance.Rmd).

Another thing to consider is whether we should be using a list or an object to
store the information pertaining to each event. I'll try to create a list at
first but maybe it would be better to have a class for events. This way, the
attributes would be much clearer to others. But really, would it be that
different if we take into account that the documentation explains the data
structure anyway? And is there any impact in the performance of the same script
using objects instead of lists?

> TODO: event data structure - list VS object

# Retrieving and parsing alternative splicing annotation
To get the most complete annotation of alternative splicing events, we'll
combine the alternative splicing annotation from many programs like MISO,
VAST-TOOLS, SUPPA and MATS. Others programs' annotation may be added later on.

The parsing of the annotation originated from different programs is explained
in the following articles:

* [MISO](Parsing MISO events.Rmd)
* [SUPPA](Parsing SUPPA events.Rmd)
* [VAST-TOOLS](Parsing VAST-TOOLS events.Rmd)
* [MATS](Parsing MATS events.Rmd)

## MISO
It's really easy to get the alternative splicing annotation used by
MISO since it's available online at
<https://miso.readthedocs.org/en/fastmiso/annotation.html> (GFF3 format)

MISO's annotation files include skipped exon (SE), mutually exclusive exon
(MXE), intron retention (RI), alternative 3' and 5' splice site (A3SS and A5SS),
alternative first and last exon (AFE and ALE) and tandem UTR. Each event type
is contained in a file with the respective acronym shown in parenthesis.

Here follows an example of an AS annotation file used in MISO:
    
chr1 | SE | gene | 16854 | 18061 | . | - | . | ID=chr1:7778:7924:-\@chr1:7096:7605:-\@chr1:6717:6918:-;Name=chr1:7778:7924:-\@chr1:7096:7605:-\@chr1:6717:6918:-;gid=chr1:7778:7924:-\@chr1:7096:7605:-\@chr1:6717:6918:-
chr1 | SE | mRNA | 16854 | 18061 | . | - | . | ID=chr1:7778:7924:-\@chr1:7096:7605:-\@chr1:6717:6918:-.A;Parent=chr1:7778:7924:-\@chr1:7096:7605:-\@chr1:6717:6918:-;gid=chr1:7778:7924:-\@chr1:7096:7605:-\@chr1:6717:6918:-
chr1 | SE | exon | 16854 | 17055 | . | - | . | ID=chr1:7778:7924:-\@chr1:7096:7605:-\@chr1:6717:6918:-.A.dn;Parent=chr1:7778:7924:-\@chr1:7096:7605:-\@chr1:6717:6918:-.A;gid=chr1:7778:7924:-\@chr1:7096:7605:-\@chr1:6717:6918:-

To generate a list of events from MISO's annotation file, we'll use:
```{r, eval=FALSE}
types <- c("AFE", "ALE", "SE", "MXE", "A5SS", "A3SS", "RI", "TandemUTR")
miso.hg19 <- lapply(types,
                    function(x) {
                        type <- paste0(
                            "/genedata/Resources/Annotations/MISO/hg19/",
                            x, ".hg19.gff3")
                        print(type)
                        read.delim(type, header=FALSE, comment.char="#",
                                   stringsAsFactors = FALSE)
                    })

misoEvents <- lapply(1:length(miso.hg19),
                     function(i) {
                         type <- types[i]
                         print(type)
                         return(parseMisoEvent(miso.hg19[[i]]))
                     })
misoEvents <- plyr::rbind.fill(misoEvents)

# Let's check the numbers
nrow(misoEvents)
sapply(types, function(type)
    sum(misoEvents$Event.type == type))

# all event types:    107178
# skipped exons:       39207
# alt. first exons:    18989
# alt. last exons:      9863
# mutually exclusive:   2721
# alt. 5 splice site:  12805
# alt. 3 splice site:  14951
# intron retention:     5986
# tandem UTR:           2656
```

One thing important to note is that ```misoEvents``` was using 11.2GB of memory
when holding all unparseable events as character strings (AFE and ALE types
have many unrecognisable events). If we don't store those, ```misoEvents```
lowers the memory ysed to 3GB. And if we don't save any unrecognised
events, it'll only use 23.9 MB of memory.

## SUPPA
We need to run the SUPPA's command generateEvents to get the alternative 
splicing annotation from a GTF file, like this:
 
```
python suppa.py generateEvents -i $GTF -o suppaEvents -e SE SS MX RI FL
```

This command generates many annotation files (ending with the `ioe` extension
and one for each event type supported), which include the alternative event
coordinates. These files look like the following:

seqname	| gene_id |	event_id |	alternative_transcripts |	total_transcripts
------- | ------- | -------- | ------------------------ | -----------------
HSCHR1_1_CTG31 | ENSG00000262826 | ENSG00000262826;RI:HSCHR1_1_CTG31:153762047:153762147-153762346:153762418:+ | ENST00000576422,ENST00000571829 | ENST00000576422,ENST00000571829,ENST00000576030,ENST00000571768,ENST00000575952,ENST00000575774,ENST00000575708,ENST00000573844
HG1350_HG959_PATCH | ENSG00000272985 | ENSG00000272985;RI:HG1350_HG959_PATCH:42384895:42385080-42385440:42385558:+ | ENST00000609291 | ENST00000609291,ENST00000609866
HG1350_HG959_PATCH | ENSG00000272985 | ENSG00000272985;RI:HG1350_HG959_PATCH:42384895:42385194-42385440:42385558:+ | ENST00000609291 | ENST00000609291,ENST00000608019

The AS event types supported by SUPPA are skipped exon (SE), mutually exclusive 
exon (MX), intron retention (RI), alternative 3' and 5' splice site (A3 and A5) 
and alternative first and last exon (AF and AL).

Let's generate a list of events from the SUPPA annotated events.
```{r, eval=FALSE}
read_path <- function(path, item, ...)
    read.delim(sprintf(path, item), ...)

suppa.path <- "/genedata/NunoA/psi_calculation/suppa/suppaEvents/hg19_%s.ioe"
types <- c("SE", "AF", "AL", "MX", "A5", "A3", "RI")
suppa.hg19 <- lapply(types, function(type) read_path(suppa.path, type))

suppaEvents <- lapply(1:length(suppa.hg19),
                      function(i) {
                          type <- types[i]
                          print(type)
                          annotation <- as.character(suppa.hg19[[i]]$event_id)
                          if (length(annotation) > 0)
                              return(parseSuppaEvent(annotation))
                      })
suppaEvents <- rbind.fill(suppaEvents)

# Let's check the numbers
nrow(suppaEvents)
sapply(c("SE", "AFE", "ALE", "MXE", "A5SS", "A3SS", "RI"), function(type)
    sum(suppaEvents$Event.type == type))

# all event types:    179108
# skipped exons:       40419
# alt. first exons:    75548
# alt. last exons:     18741
# mutually exclusive:   5114
# alt. 5 splice site:  15651
# alt. 3 splice site:  16861
# intron retention:     6774
# tandem UTR:              0
```

## MATS
After running MATS with two samples (two FASTQ or BAM files) and a GTF file
annotation of transcripts, the program returns a folder named *ASEvents* that
contains all alternative splicing events derived from the GTF file alone in
simple TXT files that are tab-delimited.

The AS event types supported by MATS are skipped exon (SE), mutually exclusive 
exon (MXE), intron retention (RI), alternative 3' and 5' splice site (A3SS and 
A5SS) and alternative first and last exon (AFE and ALE). MATS also creates files
of each event type's novel events.

The files look like this:

GeneID | geneSymbol | chr | strand | exonStart_0base | exonEnd | upstreamES | upstreamEE | downstreamES | downstreamEE
-------|------------|-----|--------|-----------------|---------|------------|------------|--------------|-------------
"ENSG00000146263" | "MMS22L" | chr6 | - | 97717982 | 97718159 | 97717777 | 97717868 | 97720579 | 97720757
"ENSG00000146263" | "MMS22L" | chr6 | - | 97681736 | 97681856 | 97678144 | 97679528 | 97694503 | 97694566
"ENSG00000146263" | "MMS22L" | chr6 | - | 97711210 | 97711324 | 97702432 | 97702609 | 97715747 | 97715878

Generating a list of events from the annotation is easy:
```{r, eval=FALSE}
read_path <- function(path, item, ...)
    read.delim(sprintf(path, item), ...)

mats.path <- "/genedata/NunoA/psi_calculation/mats_out/ASEvents/fromGTF.%s.txt"
types <- c("SE", "AFE", "ALE", "MXE", "A5SS", "A3SS", "RI")
mats.hg19 <- lapply(c(types, paste0("novelEvents.", types)), 
                     function(type) read_path(mats.path, type, row.names=NULL))

matsEvents <- lapply(1:length(mats.hg19),
                     function(i) {
                         type <- rep(types, 2)[i]
                         print(type)
                         annotation <- mats.hg19[[i]]
                         if (nrow(annotation) > 0)
                            return(parseMatsEvent(annotation, type))
                     })
matsEvents <- rbind.fill(matsEvents)

# Let's check the numbers
nrow(matsEvents)
sapply(types, function(type)
    sum(matsEvents$Event.type == type))

# all event types:    118417
# skipped exons:       37935
# alt. first exons:    51062
# alt. last exons:      8951
# mutually exclusive:   2333
# alt. 5 splice site:   4890
# alt. 3 splice site:   7706
# intron retention:     5540
# tandem UTR:              0
```

## VAST-TOOLS
VAST-TOOLS annotation comes as an extra download packed in VASTDB. They are
located in the folder named *Templates*. Each AS event type supported has its
own file with the exception of some cases where there are two files presenting 
an alternative annotation for the same event type.

GENE | EVENT | COORD | LENGTH | FullCO | COMPLEX
-----|-------|-------|--------|--------|--------
TSPAN6 | ENSG00000000003_CASSETTE1 | chrX:99885756-99885863 | 108 | chrX:99887482,99885756-99885863,99884983          | S
TSPAN6 | ENSG00000000003_CASSETTE2 | chrX:99888402-99888536 | 135 | chrX:99888928,99888402-99888536,99887565          | S
TSPAN6 | ENSG00000000003_CASSETTE3 | chrX:99891188-99891204 | 17  | chrX:99891605+99891790,99891188-99891204,99890743 | S

The event types covered by VAST-TOOLS are exon skipping (EXSK for single AS
exons, MULTI for multi AS exons and MIC for microexons), intron retention (IR)
and alternative 3' and 5' splice site (ALT3 and ALT5). There are other
VAST-TOOLS files:

- **MERGE3m** files merge all exon skipping events (those from EXSK, MULTI and MIC)
- **COMBI** files are used for the *a posteriori* pipeline; it uses all exon-exon
junctions and calls AS events based on read maps to exon-exon junctions
- **FULL** is used to decide what to print

Now, let's just generate a list of events from the annotation files. Just don't 
forget that annotation files don't have inclusion levels and the like. As the
document with more columns is RI event's annotation file with 7 columns while
all the others have 6 columns, parsed events won't show inclusion levels if they
have 7 or less columns (since there isn't data with that info... duh).

```{r, eval=FALSE}
read_path <- function(path, item1, item2, ...)
    read.delim(sprintf(path, item1, item2), stringsAsFactors = F, ...)

vastTools.path <- "/genedata/Resources/Software/vast-tools/VASTDB/Hsa/TEMPLATES/Hsa.%s.Template%s.txt"
types <- c("ALT3", "ALT5", "COMBI", "IR", "MERGE3m", "MIC", rep(c("EXSK", "MULTI"), 1))
num <- c(rep("", 6), rep(".2", 2))#, rep(".2", 2))
vastTools.hg19 <- lapply(1:length(types), 
                         function(i) read_path(vastTools.path, types[i], num[i],
                                               row.names=NULL))

vastToolsEvents <- lapply(1:length(vastTools.hg19),
                     function(i) {
                         type <- types[i]
                         print(type)
                         annotation <- vastTools.hg19[[i]]
                         if (nrow(annotation) > 0)
                            return(parseVastToolsEvent(annotation))
                     })
vastToolsEvents <- rbind.fill(vastToolsEvents)

# Let's check the numbers
nrow(vastToolsEvents)
sapply(c("SE", "A5SS", "A3SS", "RI"), function(type)
    sum(vastToolsEvents$Event.type == type))

# all event types:    444455
# skipped exons:      241116
# alt. first exons:        0
# alt. last exons:         0
# mutually exclusive:      0
# alt. 5 splice site:  15835
# alt. 3 splice site:  20860
# intron retention:   166644
# tandem UTR:              0
```

# Event representation: list, data frame, data table or object
To represent an event we started to use a list with many attributes. However,
this is not practical. A much more natural way to represent information in R is
by using a data frame. Even better, we could use a data table (a package which
makes most operations on data frames faster). Another way more akin to saving
information in object-oriented languages is by using objects. So, which of these 
ways is the best considering both memory and time?

Before anything, the time needed and memory occupied of many functions was
registred for comparison.

## MATS

So, let's first try to get MATS events to a data frame instead of a list of 
lists since MATS is the easiest to parse. There's just one thing: we need to
vectorise as much as we can! But how do we vectorise based on a condition (for
example, we want to return different based on the strand). The regular ```if```
is not vectorised, but ```ifelse``` is. The problem with this command is that 
it MUST return a value with the same length as the arguments... This is bad for
example if we want to do ```ifelse(strand == "+", junctions, rev(junctions))```
because this will return a list of junctions where the list has the same length
as the ```strand``` vector (yay!) but each element of the list has the same
length as the number of rows in junctions since it is repeating the junctions to
forcely reach that length (wait... what?).

So let's just use something simpler. The data frame will be asked for events
with plus strand and act on them accordingly. Then, the data frame will act on
the events with minus strand. Like this:
```{r, eval=FALSE}
plus <- event$Strand == "+"
event[plus, c("C1.end", "C2.start")] <- junctions[plus, ]
event[!plus, c("C1.end", "C2.start")] <- rev(junctions[!plus, ])

# maybe it's faster to fill everything and edit what's different?
# it's not much faste... actually, it takes about the same time
event[c("C1.end", "C2.start")] <- junctions
minus <- event$Strand == "-"
event[minus, c("C2.start", "C1.end")] <- junctions[minus, ]
```

Simple. But is it fast?

Type          | Time   | Memory   | Commit
--------------|--------|----------|---------
List of lists | 63.3 s | 30.4  MB | 8d9defa5
Data frame    |  0.3 s | 26.2  MB | 450f891c

There's a clearer improvement!

## SUPPA
Doing the same thing for SUPPA events.

Type          | Time   | Memory   | Commit
--------------|--------|----------|---------
List of lists | 20.5 s | 42.75 MB | 8d9defa5
Data frame    |  2.0 s | 36.4  MB | acf23a6c

## VAST-TOOLS
Now for VAST-TOOLS.

Type          | Time    | Memory | Commit
--------------|---------|--------|---------
List of lists | 377.7 s | 1   Gb | 3e702152
Data frame    |  12.9 s | 0.2 Gb | 0eb2c879

## MISO
Parsing MISO events is the second slowest of the bunch, taking around 300
seconds to parse everything. The problem here is that we need to parse the lines 
from the annotation files and to get those lines. We could try to vectorise 
any apply functions.

Let's try something like...
```{r, eval=F}
gene <- which(miso.hg19$V3 == "gene")
diff <- c(gene, nrow(miso.hg19) + 1)[2:(length(gene)+1)] - gene
groups <- unlist(sapply(1:length(diff), function(i) rep(i, c[i])))

microbenchmark(miso.split <- split(miso.hg19, groups), times = 1) # 162 seconds
```
Since it took it 308 seconds before (almost everything just for this step), we
reduced the time needed to half. But what if instead of splitting a giant data
frame we try to split minor data frames?

```{r, eval=F}
types <- c("AFE", "ALE", "SE", "MXE", "A5SS", "A3SS", "RI", "TandemUTR")
miso.hg19 <- lapply(types,
                    function(x) {
                        type <- paste0("/genedata/Resources/Annotations/MISO/hg19/", x, ".hg19.gff3")
                        print(type)
                        read.delim(type, header=FALSE, comment.char="#",
                                   stringsAsFactors = FALSE)[1:8]
                    })

misoEvents <- lapply(1:length(miso.hg19),
                     function(x) {
                         print(types[x])
                         read <- miso.hg19[[x]]
                         gene <- which(read$V3 == "gene")
                         diff <- c(gene, nrow(read) + 1)[2:(length(gene)+1)] - gene
                         groups <- unlist(sapply(1:length(diff), function(i) rep(i, diff[i])))
                         a <- split(read, groups)
                     })
```
Now the time has decreased to 34 seconds. Better but still...
Another thing which was tried (and failed) was to convert to data frame in
separate.

```{r, eval=FALSE}
file <- "/genedata/Resources/Annotations/MISO/hg19/TandemUTR.hg19.gff3"
#' Get data frame from file and split it
test1 <- function() {
    read <- read.delim(file, header=FALSE, comment.char="#",
                       stringsAsFactors = FALSE)                            # nrow:   13280
    gene <- which(read$V3 == "gene")                                        # length:  2656
    diff <- c(gene, nrow(read) + 1)[2:(length(gene)+1)] - gene              # length:  2656
    groups <- unlist(sapply(1:length(diff), function(i) rep(i, diff[i])))   # length: 13280
    res <- split(read, groups)
}

#' Get character vector from file, convert to data frame and split it
test2 <- function() {
    read <- readLines(file)
    read <- read[2:length(read)]                                            # length: 13280
    gene <- grep("\tgene\t", g, fixed = T)                                  # length:  2656
    diff <- c(gene, length(read) + 1)[2:(length(gene)+1)] - gene            # length:  2656
    groups <- unlist(sapply(1:length(diff), function(i) rep(i, diff[i])))   
    groups <- c(groups, tail(groups, 1))                                    # length: 13280
    read <- read.table(text = read)
    res <- split(read, groups)
}N

#' Get character vector from file, split them and convert to data frame
test3 <- function() {
    read <- readLines(file)
    read <- read[2:length(read)]                                            # length: 13280
    gene <- grep("\tgene\t", g, fixed = T)                                  # length:  2656
    diff <- c(gene, length(read) + 1)[2:(length(gene)+1)] - gene            # length:  2656
    groups <- unlist(sapply(1:length(diff), function(i) rep(i, diff[i])))   
    groups <- c(groups, tail(groups, 1))                                    # length: 13280
    res <- split(read, groups)
    res <- lapply(res, function(i) read.table(text =i))
}

microbenchmark(times = 1, test1(), test2(), test3())
# expr    time (ms)
# test1()  764.8314
# test2() 3201.7124
# test3() 3274.0744
```

It's interesting to note that split can be made faster if we subset the data.
For example...
```{r, eval=FALSE}
file <- "/genedata/Resources/Annotations/MISO/hg19/SE.hg19.gff3"
read <- read.delim(file, header=FALSE, comment.char="#",
                       stringsAsFactors = FALSE)
gene <- which(read$V3 == "gene")                                    
diff <- c(gene, nrow(read) + 1)[2:(length(gene)+1)] - gene           
groups <- unlist(sapply(1:length(diff), function(i) rep(i, diff[i])))

library(microbenchmark)
res <- microbenchmark(times=1,
                      split(read[[1]], groups),
                      split(read[, 1:7], groups),
                      split(read[1:7], groups),
                      split(read, groups))
library(ggplot2)
autoplot(res)
```
We could try to split indeces and access them when needed like this
```split(seq_len(nrow(read)), groups)```. This implies that we get each row
when needed but isn't that the same problem?

> TODO

Yet another thing we could try is to use a data table instead.

```{r, eval=FALSE}
file <- "/genedata/Resources/Annotations/MISO/hg19/SE.hg19.gff3"
read <- read.delim(file, header=FALSE, comment.char="#",
                       stringsAsFactors = FALSE)
gene <- which(read$V3 == "gene")
next_gene <- c(gene, nrow(read) + 1)[2:(length(gene)+1)]
diff <- next_gene - gene           
groups <- unlist(sapply(1:length(diff), function(i) rep(i, diff[i])))

library(data.table)
dt <- data.table(read)
# setkey(dt, "V9")

max_test = 1000

library(microbenchmark)
res <- microbenchmark(times=20,
                      data.frame=lapply(1:max_test, function(i) read[gene[i]:next_gene[i], ]),
                      data.table=lapply(1:max_test, function(i) dt[gene[i]:next_gene[i]]))
library(ggplot2)
autoplot(res)

# -----------------------

file <- "/genedata/Resources/Annotations/MISO/hg19/TandemUTR.hg19.gff3"
read <- read.delim(file, header=FALSE, comment.char="#",
                   stringsAsFactors = FALSE)                            # nrow:   13280
gene <- which(read$V3 == "gene")                                        # length:  2656
diff <- c(gene, nrow(read) + 1)[2:(length(gene)+1)] - gene              # length:  2656

pb <- txtProgressBar(min = 1, max = length(gene), style = 3)
res <- lapply(1:length(gene), function(i) {
    setTxtProgressBar(pb, i)
    read[gene[i]:diff[i], ]
})

# getting indexes for data tables the same way as for data frames...
# it takes 4 seconds so it's 1 second faster
library(data.table)
read <- data.table(read)
res <- lapply(1:length(gene), function(i) {
    setTxtProgressBar(pb, i)
    read[gene[i]:diff[i]]
})

# trying to split by gid...
v <- function() {
    str <- read[, tstrsplit(V9, "gid=", fixed=TRUE)]
    res <- split(read, str$V2) # try to use data table split instead!
}
```

Everything tried until now takes too much time. Let's just vectorise the
functions. It should be much faster this way. By vectorising, I mean to parse
all events at the same time, instead of splitting each event and putting them
inside a list

> TODO: increase number of AFE and ALE events included in the parsing

Type          | Time     | Memory  | Commit
--------------|----------|---------|---------
List of lists | 308.0 s  | 83.7 MB | 8d9defa5
Data frame    |   3.6 s  |  8.5 MB | 891a972b

# Getting TCGA's read counts for the annotated splicing junctions
[The Cancer Genome Atlas (TCGA)](https://tcga-data.nci.nih.gov/tcga/) is a
data repository containing clinical information, genomic characterization data, 
and high level sequence analysis of many types of tumor genomes. It's possible
to obtain read counts for splicing junctions just as read counts for exons and
isoforms.