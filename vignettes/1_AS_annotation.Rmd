---
title: "Splicing event annotation"
author: "Nuno Agostinho"
date: "`r Sys.Date()`"
output: html_document
---

The splicing events annotation is different according to the program that
creates it. The programs which allow to quantify expression levels of
alternative splicing events studied in this report are:

* [MISO](https://miso.readthedocs.org/en/fastmiso/)
* [SUPPA](https://bitbucket.org/regulatorygenomicsupf/suppa)
* [VAST-TOOLS](https://github.com/vastgroup/vast-tools)
* [MATS](http://rnaseq-mats.sourceforge.net)

It's also important to understand that the splicing events annotation may vary
according to the splicing event type. The most common splicing event types
annotated across the aforementioned programs are described in the following
image (adapted image from the
[SUPPA website](https://bitbucket.org/regulatorygenomicsupf/suppa)). One event
type that is missing from the image is the **Tandem UTR**.

![Splicing event types](http://i.imgur.com/NaRrwji.jpg)

To conciliate the output from the different programs, it was decided to parse
each splicing event ID to a R data type known as list with all the important
attributes of an event, including:

* Species
* Chromosome
* Strand
* Gene
* mRNA
* Splicing event type
* Inclusion level (PSI)

In order to store the events themselves, it's possible to characterise all the
represented splicing event types by using the boundaries of the following 4
exons:

* Upstream constitutive exon (C1)
* Upstram alternative exon (A1)
* Downstream alternative exon (A2)
* Downstream constitutive exon (C2)

The following table describes which positions should be stored to identify each
splicing event type:

Splicing event type        | C1 start | C1 end | A1 start | A1 end | A2 start | A2 end | C2 start | C2 end |
-------------------------- | :------: | :----: | :------: | :----: | :------: | :----: | :------: | :----: |
Exon skipping              |          | •      | •        | •      |          |        | •        |        |
Mutually exclusive exons   |          | •      | •        | •      | •        | •      | •        |        |
Alternative 5' splice site |          | •      |          | •      |          |        | •        |        |
Alternative 3' splice site |          | •      | •        |        |          |        | •        |        |
Retained intron            | •        | •      |          |        |          |        | •        | •      |
Alternative first exon     | •        | •      | •        | •      |          |        | •        |        |
Alternative last exon      |          | •      | •        | •      |          |        | •        | •      | 
Tandem UTR                 |          |        |          |        |          | •      | •        | •      | 

Before, I though something along the lines of:

> Note there are two glyphs in some cases. That means two alternative positions
are stored (for example, in the case of the alternative 5' splice site, the C1
end can alternatively be one of the two positions; same goes for the alt. 3'
splice site's C2 start and for the C2 end's tandem UTR).

Scratch that. It's way harder to put and manage lists inside data frames then
I figured. Now those cases have separate coordinates for the
alternative start/end positions. The alt. 5' splice site alternative junctions
are reported by C1 end and A1 end; the alt. 3' splice site alternative
junctions are reported by C2 start and A1 start; the Tandem UTR alternative
junctions are reported by C2 end and A2 end (inclusion levels are always
relative to the constitutive positions first).

Before discussing how to parse thousands and thousands of events, we should
reflect a bit on the tools available to compare different solutions that solve
the same problem and how to measure performance on R. You can read about it
in the article [Measuring performance](Measuring performance.Rmd).

> Another thing to consider is whether we should be using a list, an object or a
data frame to store the information pertaining to each event.

Procedure: I'll try to create a list at first but maybe it would be better to
have a class for events. This way, the attributes would be much clearer to
others. But really, would it be that different if we take into account that the
documentation explains the data structure anyway? And is there any impact in the
performance of the same script using objects instead of lists? Besides, data 
frames are the natural way to manipulate information in R. We'll see which is
better.

Answer: Data frames are better in terms of performance. :)

# Retrieving and parsing alternative splicing annotation
To get the most complete annotation of alternative splicing events, we'll
combine the alternative splicing annotation from many programs like MISO,
VAST-TOOLS, SUPPA and MATS. Others programs' annotation may be added later on.

The parsing of the annotation originated from different programs is explained
in the following articles:

* [MISO](Parsing MISO events.Rmd)
* [SUPPA](Parsing SUPPA events.Rmd)
* [VAST-TOOLS](Parsing VAST-TOOLS events.Rmd)
* [MATS](Parsing MATS events.Rmd)

## MISO
It's really easy to get the alternative splicing annotation used by
MISO since it's available online at
<https://miso.readthedocs.org/en/fastmiso/annotation.html> (GFF3 format)

MISO's annotation files include skipped exon (SE), mutually exclusive exon
(MXE), intron retention (RI), alternative 3' and 5' splice site (A3SS and A5SS),
alternative first and last exon (AFE and ALE) and tandem UTR. Each event type
is contained in a file with the respective acronym shown in parenthesis.

Here follows an example of an AS annotation file used in MISO:

Chr | Event type | Element | Start | End | . | Strand | . | Attributes
-----|----|------|-------|-------|---|---|---|-------
chr1 | SE | gene | 16854 | 18061 | . | - | . | ID=chr1:7778:7924:-\@chr1:7096:7605:-\@chr1:6717:6918:-;Name=chr1:7778:7924:-\@chr1:7096:7605:-\@chr1:6717:6918:-;gid=chr1:7778:7924:-\@chr1:7096:7605:-\@chr1:6717:6918:-
chr1 | SE | mRNA | 16854 | 18061 | . | - | . | ID=chr1:7778:7924:-\@chr1:7096:7605:-\@chr1:6717:6918:-.A;Parent=chr1:7778:7924:-\@chr1:7096:7605:-\@chr1:6717:6918:-;gid=chr1:7778:7924:-\@chr1:7096:7605:-\@chr1:6717:6918:-
chr1 | SE | exon | 16854 | 17055 | . | - | . | ID=chr1:7778:7924:-\@chr1:7096:7605:-\@chr1:6717:6918:-.A.dn;Parent=chr1:7778:7924:-\@chr1:7096:7605:-\@chr1:6717:6918:-.A;gid=chr1:7778:7924:-\@chr1:7096:7605:-\@chr1:6717:6918:-

To generate a list of events from MISO's annotation file, we'll use:
```{r miso-events, eval=FALSE}
types <- c("SE", "AFE", "ALE", "MXE", "A5SS", "A3SS", "RI", "TandemUTR")
types_file <- sprintf("/genedata/Resources/Annotations/MISO/hg19/%s.hg19.gff3",
                      types)
miso.hg19 <- lapply(types_file, read.delim, stringsAsFactors = FALSE,
                    comment.char="#", header=FALSE)
## TODO: ALE events are baldy formatted... they have two consecutive gene lines
## for now, remove them with this:
miso.hg19[[3]] <- miso.hg19[[3]][-c(49507, 49508), ]

misoEvents <- lapply(miso.hg19, parseMisoEvent)
misoEvents <- plyr::rbind.fill(misoEvents)

# Let's check the numbers
nrow(misoEvents)
sapply(types, function(type)
    sum(misoEvents$Event.type == type))
rm(types, types_file)

# all event types:    107178
# skipped exons:       39207
# alt. first exons:    18989
# alt. last exons:      9863
# mutually exclusive:   2721
# alt. 5 splice site:  12805
# alt. 3 splice site:  14951
# intron retention:     5986
# tandem UTR:           2656
```

One thing important to note is that ```misoEvents``` was using 11.2GB of memory
when holding all unparseable events as character strings (AFE and ALE types
have many unrecognisable events). If we don't store those, ```misoEvents```
lowers the memory ysed to 3GB. And if we don't save any unrecognised
events, it'll only use 23.9 MB of memory.

## SUPPA
We need to run the SUPPA's command generateEvents to get the alternative 
splicing annotation from a GTF file, like this:

```
python suppa.py generateEvents -i $GTF -o suppaEvents -e SE SS MX RI FL
```

This command generates many annotation files (ending with the `ioe` extension
and one for each event type supported), which include the alternative event
coordinates. These files look like the following:

seqname	| gene_id |	event_id |	alternative_transcripts |	total_transcripts
------- | ------- | -------- | ------------------------ | -----------------
HSCHR1_1_CTG31 | ENSG00000262826 | ENSG00000262826;RI:HSCHR1_1_CTG31:153762047:153762147-153762346:153762418:+ | ENST00000576422,ENST00000571829 | ENST00000576422,ENST00000571829,ENST00000576030,ENST00000571768,ENST00000575952,ENST00000575774,ENST00000575708,ENST00000573844
HG1350_HG959_PATCH | ENSG00000272985 | ENSG00000272985;RI:HG1350_HG959_PATCH:42384895:42385080-42385440:42385558:+ | ENST00000609291 | ENST00000609291,ENST00000609866
HG1350_HG959_PATCH | ENSG00000272985 | ENSG00000272985;RI:HG1350_HG959_PATCH:42384895:42385194-42385440:42385558:+ | ENST00000609291 | ENST00000609291,ENST00000608019

The AS event types supported by SUPPA are skipped exon (SE), mutually exclusive 
exon (MX), intron retention (RI), alternative 3' and 5' splice site (A3 and A5) 
and alternative first and last exon (AF and AL).

Let's generate a list of events from the SUPPA annotated events.
```{r suppa-events, eval=FALSE}
types <- c("SE", "AF", "AL", "MX", "A5", "A3", "RI")
types_file <- sprintf(
    "~/Documents/psi_calculation/suppa/suppaEvents/hg19_%s.ioe", types)
suppa.hg19 <- lapply(types_file, read.delim, stringsAsFactors = FALSE,
                     comment.char="#", header=TRUE)

eventsID <- lapply(suppa.hg19, "[[", "event_id")
suppaEvents <- lapply(eventsID, parseSuppaEvent)
suppaEvents <- plyr::rbind.fill(suppaEvents)

# Let's check the numbers
nrow(suppaEvents)
sapply(c("SE", "AFE", "ALE", "MXE", "A5SS", "A3SS", "RI"), function(type)
    sum(suppaEvents$Event.type == type))
rm(types, types_file, eventsID)

# all event types:    179108
# skipped exons:       40419
# alt. first exons:    75548
# alt. last exons:     18741
# mutually exclusive:   5114
# alt. 5 splice site:  15651
# alt. 3 splice site:  16861
# intron retention:     6774
# tandem UTR:              0
```

## MATS
After running MATS with two samples (two FASTQ or BAM files) and a GTF file
annotation of transcripts, the program returns a folder named *ASEvents* that
contains all alternative splicing events derived from the GTF file alone in
simple TXT files that are tab-delimited.

The AS event types supported by MATS are skipped exon (SE), mutually exclusive 
exon (MXE), intron retention (RI), alternative 3' and 5' splice site (A3SS and 
A5SS) and alternative first and last exon (AFE and ALE). MATS also creates files
of each event type's novel events.

The files look like this:

GeneID | geneSymbol | chr | strand | exonStart_0base | exonEnd | upstreamES | upstreamEE | downstreamES | downstreamEE
-------|------------|-----|--------|-----------------|---------|------------|------------|--------------|-------------
"ENSG00000146263" | "MMS22L" | chr6 | - | 97717982 | 97718159 | 97717777 | 97717868 | 97720579 | 97720757
"ENSG00000146263" | "MMS22L" | chr6 | - | 97681736 | 97681856 | 97678144 | 97679528 | 97694503 | 97694566
"ENSG00000146263" | "MMS22L" | chr6 | - | 97711210 | 97711324 | 97702432 | 97702609 | 97715747 | 97715878

Generating a list of events from the annotation is easy:
```{r mats-events, eval=FALSE}
types <- c("SE", "AFE", "ALE", "MXE", "A5SS", "A3SS", "RI")
types_file <- sprintf(
    "~/Documents/psi_calculation/mats_out/ASEvents/fromGTF.%s.txt",
    c(types, paste0("novelEvents.", types)))
mats.hg19 <- lapply(types_file, read.delim, stringsAsFactors = FALSE,
                    comment.char="#", header=TRUE)

matsEvents <- lapply(seq_along(mats.hg19),
                     function(i) {
                         type <- rep(types, 2)[i]
                         annotation <- mats.hg19[[i]]
                         if (nrow(annotation) > 0)
                             return(parseMatsEvent(annotation, type))
                     })
matsEvents <- plyr::rbind.fill(matsEvents)

# Let's check the numbers
nrow(matsEvents)
sapply(types, function(type)
    sum(matsEvents$Event.type == type))
rm(types, types_file)

# all event types:    118417
# skipped exons:       37935
# alt. first exons:    51062
# alt. last exons:      8951
# mutually exclusive:   2333
# alt. 5 splice site:   4890
# alt. 3 splice site:   7706
# intron retention:     5540
# tandem UTR:              0
```

## VAST-TOOLS
VAST-TOOLS annotation comes as an extra download packed in VASTDB. They are
located in the folder named *Templates*. Each AS event type supported has its
own file with the exception of some cases where there are two files presenting 
an alternative annotation for the same event type.

GENE | EVENT | COORD | LENGTH | FullCO | COMPLEX
-----|-------|-------|--------|--------|--------
TSPAN6 | ENSG00000000003_CASSETTE1 | chrX:99885756-99885863 | 108 | chrX:99887482,99885756-99885863,99884983          | S
TSPAN6 | ENSG00000000003_CASSETTE2 | chrX:99888402-99888536 | 135 | chrX:99888928,99888402-99888536,99887565          | S
TSPAN6 | ENSG00000000003_CASSETTE3 | chrX:99891188-99891204 | 17  | chrX:99891605+99891790,99891188-99891204,99890743 | S

The event types covered by VAST-TOOLS are exon skipping (EXSK for single AS
exons, MULTI for multi AS exons and MIC for microexons), intron retention (IR)
and alternative 3' and 5' splice site (ALT3 and ALT5). There are other
VAST-TOOLS files:

- **MERGE3m** files merge all exon skipping events (those from EXSK, MULTI and 
MIC)
- **COMBI** files are used for the *a posteriori* pipeline which uses all 
exon-exon (EE) junctions and calls AS events based on read maps to EE junctions
- **FULL** is used to decide what to print

Now, let's just generate a list of events from the annotation files. Just don't 
forget that annotation files don't have inclusion levels and the like. As the
document with more columns is RI event's annotation file with 7 columns while
all the others have 6 columns, parsed events won't show inclusion levels if they
have 7 or less columns (since there isn't data with that info... duh).

```{r vast-tools-events, eval=FALSE}
types <- c("ALT3", "ALT5", "COMBI", "IR", "MERGE3m", "MIC",
           rep(c("EXSK", "MULTI"), 1))
types_file <- sprintf(
    "/genedata/Resources/Software/vast-tools/VASTDB/Hsa/TEMPLATES/Hsa.%s.Template%s.txt",
    types, c(rep("", 6), rep(".2", 2))#, rep(".2", 2))
)

vastTools.hg19 <- lapply(types_file, read.delim, stringsAsFactors = FALSE,
                         comment.char="#", header=TRUE)

vastToolsEvents <- lapply(seq_along(vastTools.hg19),
                          function(i) {
                              type <- types[i]
                              print(type)
                              annotation <- vastTools.hg19[[i]]
                              if (nrow(annotation) > 0)
                                  return(parseVastToolsEvent(annotation))
                          })
vastToolsEvents <- plyr::rbind.fill(vastToolsEvents)

# Let's check the numbers
nrow(vastToolsEvents)
sapply(c("SE", "A5SS", "A3SS", "RI"), function(type)
    sum(vastToolsEvents$Event.type == type))
rm(types, types_file)
# all event types:    444455
# skipped exons:      241116
# alt. first exons:        0
# alt. last exons:         0
# mutually exclusive:      0
# alt. 5 splice site:  15835
# alt. 3 splice site:  20860
# intron retention:   166644
# tandem UTR:              0
```

# Event representation: list, data frame, data table or object
To represent an event we started to use a list with many attributes. However,
this is not practical. A much more natural way to represent information in R is
by using a data frame. Even better, we could use a data table (a package which
makes most operations on data frames faster). Another way more akin to saving
information in object-oriented languages is by using objects. So, which of these 
ways is the best considering both memory and time?

Before anything, the time needed and memory occupied of many functions was
registred for comparison.

## MATS

So, let's first try to get MATS events to a data frame instead of a list of 
lists since MATS is the easiest to parse. There's just one thing: we need to
vectorise as much as we can! But how do we vectorise based on a condition (for
example, we want to return different based on the strand). The regular ```if```
is not vectorised, but ```ifelse``` is. The problem with this command is that 
it MUST return a value with the same length as the arguments... This is bad for
example if we want to do ```ifelse(strand == "+", junctions, rev(junctions))```
because this will return a list of junctions where the list has the same length
as the ```strand``` vector (yay!) but each element of the list has the same
length as the number of rows in junctions since it is repeating the junctions to
forcely reach that length (wait... what?).

So let's just use something simpler. The data frame will be asked for events
with plus strand and act on them accordingly. Then, the data frame will act on
the events with minus strand. Like this:
```{r, eval=FALSE}
plus <- event$Strand == "+"
event[plus, c("C1.end", "C2.start")] <- junctions[plus, ]
event[!plus, c("C1.end", "C2.start")] <- rev(junctions[!plus, ])

# maybe it's faster to fill everything and edit what's different?
# it's not much faste... actually, it takes about the same time
event[c("C1.end", "C2.start")] <- junctions
minus <- event$Strand == "-"
event[minus, c("C2.start", "C1.end")] <- junctions[minus, ]
```

Simple. But is it fast?

Type          | Time   | Memory   | Commit
--------------|--------|----------|---------
List of lists | 63.3 s | 30.4  MB | 8d9defa5
Data frame    |  0.3 s | 26.2  MB | 450f891c

There's a clearer improvement!

## SUPPA
Doing the same thing for SUPPA events.

Type          | Time   | Memory   | Commit
--------------|--------|----------|---------
List of lists | 20.5 s | 42.75 MB | 8d9defa5
Data frame    |  2.0 s | 36.4  MB | acf23a6c

## VAST-TOOLS
Now for VAST-TOOLS.

Type          | Time    | Memory | Commit
--------------|---------|--------|---------
List of lists | 377.7 s | 1   Gb | 3e702152
Data frame    |  12.9 s | 0.2 Gb | 0eb2c879

## MISO
Parsing MISO events is the second slowest of the bunch, taking around 300
seconds to parse everything. The problem here is that we need to parse the lines 
from the annotation files and to get those lines. We could try to vectorise 
any apply functions.

Let's try something like...
```{r, eval=F}
gene <- which(miso.hg19$V3 == "gene")
diff <- c(gene, nrow(miso.hg19) + 1)[2:(length(gene)+1)] - gene
groups <- unlist(sapply(seq_along(diff), function(i) rep(i, c[i])))

microbenchmark(miso.split <- split(miso.hg19, groups), times = 1) # 162 seconds
```
Since it took it 308 seconds before (almost everything just for this step), we
reduced the time needed to half. But what if instead of splitting a giant data
frame we try to split minor data frames?

```{r, eval=F}
types <- c("AFE", "ALE", "SE", "MXE", "A5SS", "A3SS", "RI", "TandemUTR")
miso.hg19 <- lapply(types,
                    function(x) {
                        type <- paste0(
                            "/genedata/Resources/Annotations/MISO/hg19/",
                            x, ".hg19.gff3")
                        print(type)
                        read.delim(type, header=FALSE, comment.char="#",
                                   stringsAsFactors = FALSE)[1:8]
                    })

misoEvents <- lapply(seq_along(miso.hg19),
                     function(x) {
                         print(types[x])
                         read <- miso.hg19[[x]]
                         gene <- which(read$V3 == "gene")
                         diff <- c(gene, nrow(read) + 1)[2:(length(gene)+1)] - gene
                         groups <- unlist(sapply(seq_along(diff),
                                                 function(i) rep(i, diff[i])))
                         a <- split(read, groups)
                     })
```
Now the time has decreased to 34 seconds. Better but still...
Another thing which was tried (and failed) was to convert to data frame in
separate.

```{r, eval=FALSE}
file <- "/genedata/Resources/Annotations/MISO/hg19/TandemUTR.hg19.gff3"
#' Get data frame from file and split it
test1 <- function() {
    read <- read.delim(file, header=FALSE, comment.char="#",
                       stringsAsFactors = FALSE)                            # nrow:   13280
    gene <- which(read$V3 == "gene")                                        # length:  2656
    diff <- c(gene, nrow(read) + 1)[2:(length(gene)+1)] - gene              # length:  2656
    groups <- unlist(sapply(seq_along(diff), function(i) rep(i, diff[i])))   # length: 13280
    res <- split(read, groups)
}

#' Get character vector from file, convert to data frame and split it
test2 <- function() {
    read <- readLines(file)
    read <- read[2:length(read)]                                            # length: 13280
    gene <- grep("\tgene\t", g, fixed = T)                                  # length:  2656
    diff <- c(gene, length(read) + 1)[2:(length(gene)+1)] - gene            # length:  2656
    groups <- unlist(sapply(seq_along(diff), function(i) rep(i, diff[i])))   
    groups <- c(groups, tail(groups, 1))                                    # length: 13280
    read <- read.table(text = read)
    res <- split(read, groups)
}N

#' Get character vector from file, split them and convert to data frame
test3 <- function() {
    read <- readLines(file)
    read <- read[2:length(read)]                                            # length: 13280
    gene <- grep("\tgene\t", g, fixed = T)                                  # length:  2656
    diff <- c(gene, length(read) + 1)[2:(length(gene)+1)] - gene            # length:  2656
    groups <- unlist(sapply(seq_along(diff), function(i) rep(i, diff[i])))   
    groups <- c(groups, tail(groups, 1))                                    # length: 13280
    res <- split(read, groups)
    res <- lapply(res, function(i) read.table(text =i))
}

microbenchmark(times = 1, test1(), test2(), test3())
# expr    time (ms)
# test1()  764.8314
# test2() 3201.7124
# test3() 3274.0744
```

It's interesting to note that split can be made faster if we subset the data.
For example...
```{r, eval=FALSE}
file <- "/genedata/Resources/Annotations/MISO/hg19/SE.hg19.gff3"
read <- read.delim(file, header=FALSE, comment.char="#",
                   stringsAsFactors = FALSE)
gene <- which(read$V3 == "gene")                                    
diff <- c(gene, nrow(read) + 1)[2:(length(gene)+1)] - gene           
groups <- unlist(sapply(seq_along(diff), function(i) rep(i, diff[i])))

library(microbenchmark)
res <- microbenchmark(times=1,
                      split(read[[1]], groups),
                      split(read[, 1:7], groups),
                      split(read[1:7], groups),
                      split(read, groups))
library(ggplot2)
autoplot(res)
```
We could try to split indeces and access them when needed like this
```split(seq_len(nrow(read)), groups)```. This implies that we get each row
when needed but isn't that the same problem?

> TODO

Yet another thing we could try is to use a data table instead.

```{r, eval=FALSE}
file <- "/genedata/Resources/Annotations/MISO/hg19/SE.hg19.gff3"
read <- read.delim(file, header=FALSE, comment.char="#",
                   stringsAsFactors = FALSE)
gene <- which(read$V3 == "gene")
next_gene <- c(gene, nrow(read) + 1)[2:(length(gene)+1)]
diff <- next_gene - gene           
groups <- unlist(sapply(seq_along(diff), function(i) rep(i, diff[i])))

library(data.table)
dt <- data.table(read)
# setkey(dt, "V9")

max_test = 1000

library(microbenchmark)
res <- microbenchmark(times=20,
                      data.frame=lapply(1:max_test, function(i)
                          read[gene[i]:next_gene[i], ]),
                      data.table=lapply(1:max_test, function(i)
                          dt[gene[i]:next_gene[i]]))
library(ggplot2)
autoplot(res)

# -----------------------

file <- "/genedata/Resources/Annotations/MISO/hg19/TandemUTR.hg19.gff3"
read <- read.delim(file, header=FALSE, comment.char="#",
                   stringsAsFactors = FALSE)               # nrow:   13280
gene <- which(read$V3 == "gene")                           # length:  2656
diff <- c(gene, nrow(read) + 1)[2:(length(gene)+1)] - gene # length:  2656

pb <- txtProgressBar(min = 1, max = length(gene), style = 3)
res <- lapply(seq_along(gene), function(i) {
    setTxtProgressBar(pb, i)
    read[gene[i]:diff[i], ]
})

# getting indexes for data tables the same way as for data frames...
# it takes 4 seconds so it's 1 second faster
library(data.table)
read <- data.table(read)
res <- lapply(seq_along(gene), function(i) {
    setTxtProgressBar(pb, i)
    read[gene[i]:diff[i]]
})

# trying to split by gid...
v <- function() {
    str <- read[, tstrsplit(V9, "gid=", fixed=TRUE)]
    res <- split(read, str$V2) # try to use data table split instead!
}
```

Everything tried until now takes too much time. Let's just vectorise the
functions. It should be much faster this way. By vectorising, I mean to parse
all events at the same time, instead of splitting each event and putting them
inside a list

> TODO: increase number of AFE and ALE events included in the parsing

Type          | Time     | Memory  | Commit
--------------|----------|---------|---------
List of lists | 308.0 s  | 83.7 MB | 8d9defa5
Data frame    |   3.6 s  |  8.5 MB | 891a972b

# Match events from different programs
To easily match events coming from different programs, we simply need to look at
specific columns. For example, to check if an exon skipping is the same, we just
need to check C1 end, A1 start + end and A2 start (for other event types, check
table at the top of this document).

```{r match-events, eval = FALSE}
# Timing the subset of one specifing event type
microbenchmark(
    matsEvents[matsEvents$Event.type == "SE", ],
    filter(matsEvents, Event.type == "SE")
)

# More interestingly, let's see how fast can we subset all event types
microbenchmark(mats <- split(matsEvents, matsEvents$Event.type),
               mats <- dlply(matsEvents, .(Event.type)))

# SUPPA events' chromosome identifier differs from the rest (let's add "chr")
suppaEvents$Chromosome <- paste0("chr", suppaEvents$Chromosome)

# MATS' skipping exon events don't match if we don't add one
matsEvents[matsEvents$Event.type == "SE", ]$A1.start <- as.numeric(
    matsEvents[matsEvents$Event.type == "SE", ]$A1.start) + 1
matsEvents[matsEvents$Event.type == "SE", ]$C2.start <- as.numeric(
    matsEvents[matsEvents$Event.type == "SE", ]$C2.start) + 1

# All right... let's separate all events based on program and event type
library(dplyr)
library(plyr)
events <- list(suppaEvents, matsEvents, misoEvents, vastToolsEvents)
events <- rbind.fill(events)
events <- dlply(events, .(Event.type))
events <- lapply(events, dlply, .(Program))
id <- names(events)

# Now, for each event type, get the matching indexes based on the common attributes of each event type

#' a <- fullJoinTables(events$SE,
#'     c("Strand", "C1.end", "A1.end", "A1.start"), c(F,T,T,T))
fullJoinTables <- function(tables, by = NULL, toNumeric = FALSE) {
    joined <- getNumerics(tables[[1]], by, toNumeric)
    # Only join tables if there are more than one
    if (length(tables) > 1) {
        for (i in 2:length(tables)) {
            table <- getNumerics(tables[[i]], by, toNumeric)
            joined <- full_join(joined, table, by)
        }
    }
    return(joined)
}

#' getNumerics(vast, by = c("Strand", "C1.end", "A1.end", "A1.start"),
#'     toNumeric = c(F,T,T,T))
getNumerics <- function(table, by = NULL, toNumeric = FALSE) {
    # Check which elements are lists of specified length
    bool <- TRUE
    for (each in by)
        bool <- bool & vapply(table[[each]], length, integer(1)) == 1
    
    table <- table[bool, ]
    # Convert elements to numeric
    conv <- by[toNumeric]
    table[conv] <- as.numeric(as.character(unlist(table[conv])))
    return(table)
}

# Join all given tables together based on given parameters
types <- names(events)
joint <- lapply(types, function(type, events) {
    print(type)
    join <- function(...) fullJoinTables(events[[type]], ...)
    
    res <- switch(type,
                  "SE"   = join(c("Strand", "Chromosome", 
                                  "C1.end", "A1.start",
                                  "A1.end", "C2.start"),
                                c(rep(FALSE, 2), rep(TRUE, 4))),
                  "A3SS" = join(c("Strand", "Chromosome",
                                  "C1.end", "C2.start",
                                  "A1.start"), c(rep(FALSE, 2), rep(TRUE, 3))),
                  "A5SS" = join(c("Strand", "Chromosome",
                                  "C1.end", "C2.start", "A1.end"),
                                c(rep(FALSE, 2), rep(TRUE, 3))),
                  "AFE"  = join(c("Strand", "Chromosome", 
                                  "C1.start", "C1.end",
                                  "A1.start", "A1.end",
                                  "C2.start"), c(rep(FALSE, 2), rep(TRUE, 5))),
                  "ALE"  = join(c("Strand", "Chromosome", "C1.end",
                                  "A1.start", "A1.end",
                                  "C2.start", "C2.end"), c(rep(FALSE, 2), rep(TRUE, 5))),
                  "MXE"  = join(c("Strand", "Chromosome", "C1.end",
                                  "A1.start", "A1.end",
                                  "A2.start", "A2.end", "C2.start"), 
                                c(rep(FALSE, 2), rep(TRUE, 6))),
                  "TandemUTR" = join(c("Strand", "Chromosome",
                                       "C2.start", "C2.end",
                                       "A1.end"),
                                     c(rep(FALSE, 2), rep(TRUE, 3))),
                  "RI"   = join(c("Strand", "Chromosome", 
                                  "C1.start", "C1.end",
                                  "C2.start", "C2.end"), 
                                c(rep(FALSE, 2), rep(TRUE, 4))))
    return(res)
}, events)
names(joint) <- types

# Okay! Let's check how many overlaps there are and how many events are unique
j <- joint$SE[grep("Program", names(joint$SE))]
nas <- !is.na(j)
mats <- which(nas[ , 1])
miso <- which(nas[ , 2])
suppa <- which(nas[ , 3])
vast <- which(nas[ , 4])

es <- list(mats = mats, miso = miso, suppa = suppa, vast = vast)
for (e1 in seq_along(es)) {
    for (e2 in seq_along(es)) {
        print(paste(names(es)[[e1]], names(es)[[e2]]))
        print(length(intersect(es[[e1]], es[[e2]])))
    }
}
```

# Getting TCGA's read counts for the annotated splicing junctions
[The Cancer Genome Atlas (TCGA)](https://tcga-data.nci.nih.gov/tcga/) is a
data repository containing clinical information, genomic characterization data, 
and high level sequence analysis of many types of tumor genomes. It's possible
to obtain read counts for splicing junctions just as read counts for exons and
isoforms. Unfortunately, these reads are separated in individual files per
sample... which means we need to join these files into one.

Let's try to do something with the junction quantification files inside the
`data/junctions` folder. First of all, let's read the files:

```{r TCGA junctions, eval = FALSE}
path <- "data/junctions/"
junctions <- paste0(path, list.files(path))
junctions.files <- lapply(junctions, read.delim2, stringsAsFactors = FALSE)

# It's important to note the first column is always the same in these files
junctionsID <- junctions.files[[1]]$junction
all(junctionsID == junctions.files[[2]]$junction)
all(junctionsID == junctions.files[[3]]$junction)

junctions <- cbind(junctions.files[[1]]$raw_counts,
                   junctions.files[[2]]$raw_counts,
                   junctions.files[[2]]$raw_counts)
row.names(junctions) <- junctionsID
```

All right! Now let's search for pairs (only in skipping exon events, for now)!
```{r TCGA/events match, eval=FALSE}
chr <- joint$SE$Chromosome
strand <- joint$SE$Strand
c1 <- joint$SE$C1.end
a1 <- joint$SE$A1.start
a2 <- joint$SE$A1.end
c2 <- joint$SE$C2.start

id_incA <- sprintf("%s:%s:%s,%s:%s:%s", chr, ifelse(c1<a1, c1, a1), strand,
                   chr, ifelse(c1<a1, a1, c1), strand)
id_incB <- sprintf("%s:%s:%s,%s:%s:%s", chr, ifelse(a2<c2, a2, c2), strand,
                   chr, ifelse(a2<c2, c2, a2), strand)
id_excl <- sprintf("%s:%s:%s,%s:%s:%s", chr, ifelse(c1<c2, c1, c2), strand, 
                   chr, ifelse(c1<c2, c2, c1), strand)

index_incA <- fastmatch::fmatch(id_incA, junctionsID)
index_incB <- fastmatch::fmatch(id_incB, junctionsID)
index_excl <- fastmatch::fmatch(id_excl, junctionsID)

inclusion <- (junctions[index_incA, ] + junctions[index_incB, ]) / 2
total <- junctions[index_excl, ] + inclusion
psi <- inclusion/total

# How many pairs?
nrow(psi[complete.cases(psi), ])
```

Only a few paired, it seems. We got the inclusion levels of 16711 out of 271998
skipping exon events. My PI says that is quite all right but I was expecting for
a number a bit higher.

To learn about data retrieval from TCGA, head to the vignette `TCGA data.Rmd`.
It also discusses how to integrate the different types of data available, like
cross-linking clinical data with sequentiation files.